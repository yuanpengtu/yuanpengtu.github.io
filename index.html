<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <style>
    .tooltip {
      position: relative;
    }

    .tooltip .tooltiptext {
      visibility: hidden;
      width: 100px;
      background-color: white;
      color: #000;
      text-align: center;
      border-radius: 6px;
      padding: 5px 0;

      /* Position the tooltip */
      position: absolute;
      z-index: 1;
      top: -5px;
      left: 70%;
    }

    .tooltip:hover .tooltiptext {
      visibility: visible;
      text-align: center;
      font-size: 36px;
    }

    .tooltip .tooltiptext a {
      color: #000;
    }
  </style>

	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
	
		<title>Kuan Fang</title>
			
    <!-- <link rel="icon" type="image/png" href="images/su_icon_1color.png"/> -->
    <link rel="icon" type="image/png" href="images/berkeley_eecs_icon.png"/>

		<!-- CSS -->
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"/>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen"/>
    <link rel="stylesheet" href="css/jquery.popup.css" type="text/css"/>
		<!-- ENDS CSS -->

    <!-- Global site tag (gtag.js) -Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-47054450-2"></script>
      <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

          gtag('config', 'UA-47054450-2');
      </script>
    <!-- End Global site tag (gtag.js) -Google Analytics -->

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MQ25LJL');</script>
    <!-- End Google Tag Manager -->

	</head>	

	<body>
    <div class="section">
      <table id="personal">
        <tr>
          <td id="basic">
            <div>
              <br/>
              <div class="tooltip" align="center">
                <h1>Kuan Fang</h1>
                <!-- <span class="tooltiptext"><a href="https://en.wiktionary.org/wiki/%E6%88%BF" target="_blank">房</a> <a href="https://en.wiktionary.org/wiki/%E5%AF%AC" target="_blank">宽</a></span> -->
              </div>
              <br/>
              <h2><b>Email</b>: kuanfang [at] berkeley [dot] edu</h2>
            </div>
          </td>
        </tr>

        <tr>

          <td id="bio">
            <div>
                I am a third-year M.S. student in <a href="https://vill.tongji.edu.cn/">VILL Lab</a> of College of Electronic and Information Engineering, Tongji University supervised by <a href="https://vill.tongji.edu.cn/cy/fzr.htm">Prof.Cairong Zhao</a>. Prior to this, I received my B.E. degree from <a href="images/tsinghua.jpg">Tongji University</a>. I will join The University of Hong Kong for PhD study in Sep 2023, fortunately under the supervision of <a href="https://hszhao.github.io/">Prof. Hengshuang Zhao</a>.

                <br/>
                <br/>

                <br/><br/>
                <h2>
                  <a href="typ.pdf">CV</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://scholar.google.com.sg/citations?hl=en&user=a70oH2wAAAAJ">Google Scholar</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://github.com/yuanpengtu/">GitHub</a>
                  &nbsp;&nbsp;|&nbsp;&nbsp;
                  <a href="https://twitter.com/YuanpengT">Twitter</a>
                </h2>
            </div>
          </td>

          <td id="photo">
            <div>
              <img src="yuanpengtu.jpg"/>
            </div>
          </td>

        </tr>

      </table>
    </div>

    <div id="info">
    <div class="section">
      <h1>Research</h1>
      <!-- <br/> -->
      <tr> 
        My research aims to enable robots to solve diverse and complex tasks in unstructured environments. To that end, I work towards building scalable data-driven methods for perception and control, with a focus on two axes: (1) How to automate data collection through the generation of feasible, diverse, and useful tasks in simulation and the real world; and correspondingly, (2) how to acquire general-purpose skills that are robust, adaptable, and extensible for efficiently solving novel long-horizon tasks.
      </tr>
    </div>



    <div id="info">

      <div class="section">
        <h1>News</h1><br/>

        <ul>
          <li>[05/2022] I am awarded the Outstanding graduates in Shanghai.</li>
          <li>[03/2022] I join QCraft as a research intern.</li>        
          <li>[02/2022] Two papers on noisy label learning are accepted by CVPR 2023.</li>
          <li>[05/2022] We won the third place in <a href="https://codalab.lisn.upsaclay.fr/competitions/6781#learn_the_details">OOD-CV</a> competition in ECCV 2022.</li>
          <li>[05/2022] One paper on noisy label learning is accepted by ECCV 2022 workshop.</li>
          <li>[08/2021] I join Tencent Youtu Lab as a research intern.</li>
          <li>[08/2021] One paper on person re-identificaition is accepted by IEEE Transaction on Image Processing 2021.</li>
        </ul>
      </div>

      <!-- <div class="section"> -->
      <!--   <h1>Preprints</h1><br/> -->
      <!--   <table id="publications"> -->
      <!--  -->
      <!--     <tr> -->
      <!--       <td id="publications-image"> -->
      <!--         <img src="papers/images/atr.gif"/> -->
      <!--       </td> -->
      <!--       <td id="publications-info"> -->
      <!--         <p> -->
      <!--           <span id="paper"> -->
      <!--             Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks -->
      <!--           </span> -->
      <!--           <br/> -->
      <!--           <span id="author"> -->
      <!--             <span id="author-kuan">Kuan Fang*</span>, -->
      <!--             Toki Migimatsu*,  -->
      <!--             Ajay Mandlekar,  -->
      <!--             Li Fei-Fei,  -->
      <!--             Jeannette Bohg -->
      <!--           </span> -->
      <!--           <br/> -->
      <!--           ArXiv -->
      <!--           <br/> -->
      <!--           <a href="https://arxiv.org/pdf/2211.06134.pdf">PDF</a> | -->
      <!--           <a href="https://sites.google.com/view/active-task-randomization">Website</a> | -->
      <!--           <a href="bibtex/atr2022.bib">BibTex</a> -->
      <!--         </p> -->
      <!--       </td> -->
      <!--     </tr> -->
      <!--  -->
      <!--   </table> -->
      <!-- </div> -->

      <div class="section">
        <h1>Publications</h1><br/>
        <table id="publications">
          <tr>
            <td id="publications-image">
              <img src="papers/images/atr.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang*</span>,
                  Toki Migimatsu*, 
                  Ajay Mandlekar, 
                  Li Fei-Fei, 
                  Jeannette Bohg
                </span>
                <br/>
                arXiv Preprint
                <br/>
                <a href="https://arxiv.org/pdf/2211.06134.pdf">PDF</a> |
                <a href="https://sites.google.com/view/active-task-randomization">Website</a> |
                <a href="bibtex/atr2022.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/flap.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>,
                  Patrick Yin, 
                  Ashvin Nair, 
                  Homer Walke, 
                  Gengchen Yan,
                  Sergey Levine
                </span>
                <br/>
                <p> CoRL 2022 <span style="color:DarkRed;"><b>(Oral Presentation)</b></span></p>
                <!-- <br/> -->
                <a href="https://arxiv.org/pdf/2210.06601.pdf">PDF</a> |
                <a href="https://sites.google.com/view/project-flap">Website</a> |
                <a href="https://github.com/kuanfang/flap">Code</a> |
                <a href="bibtex/fang2022flap.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/ptp.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang*</span>,
                  Patrick Yin*, 
                  Ashvin Nair, 
                  Sergey Levine
                  (* indicates equal contribution)
                </span>
                <br/>
                IROS 2022
                <br/>
                <a href="https://arxiv.org/pdf/2205.08129.pdf">PDF</a> |
                <a href="https://sites.google.com/view/planning-to-practice/">Website</a> |
                <a href="https://github.com/patrickhaoy/ptp">Code</a> |
                <a href="bibtex/fang2022ptp.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/slide.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Discovering Generalizable Skills via Automated Generation of Diverse Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>,
                  Yuke Zhu,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                RSS 2021
                <br/>
                <a href="https://arxiv.org/pdf/2106.13935.pdf">PDF</a> |
                <a href="https://sites.google.com/view/rss-slide/">Website</a> |
                <a href="bibtex/fang2021slide.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/giga.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations
                </span>
                <br/>
                <span id="author">
                  Zhenyu Jiang, 
                  Yifeng Zhu, 
                  Maxwell Svetlik, 
                  <span id="author-kuan">Kuan Fang</span>,
                  Yuke Zhu,
                </span>
                <br/>
                RSS 2021
                <br/>
                <a href="https://arxiv.org/pdf/2104.01542.pdf">PDF</a> |
                <a href="https://sites.google.com/view/rpl-giga2021/">Website</a> |
                <a href="https://github.com/UT-Austin-RPL/GIGA">Code</a> |
                <a href="bibtex/jiang2021giga.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/apt-gen.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Adaptive Procedural Task Generation for Hard-Exploration Problems
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>,
                  Yuke Zhu,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                ICLR 2021
                <br/>
                <a href="https://arxiv.org/pdf/2007.00350.pdf">PDF</a> |
                <a href="https://apt-gen.github.io/">Website</a> |
                <a href="bibtex/fang2020aptgen.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/keto.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  KETO: Learning Keypoint Representations for Tool Manipulation
                </span>
                <br/>
                <span id="author">
                  Zengyi Qin,
                  <span id="author-kuan">Kuan Fang</span>,
                  Yuke Zhu,
                  Li Fei-Fei,
                  Silvio Savarese
                </span>
                <br/>
                ICRA 2020
                <br/>
                <a href="https://arxiv.org/pdf/1910.11977.pdf">PDF</a> |
                <a href="https://sites.google.com/view/ke-to">Website</a> |
                <a href="https://github.com/stanfordvl/keto">Code</a> |
                <a href="bibtex/qin2019keto.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/cavin.gif"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>,
                  Yuke Zhu,
                  Animesh Garg,
                  Silvio Savarese,
                  Li Fei-Fei
                </span>
                <br/>
                <p> CoRL 2019 <span style="color:DarkRed;"><b>(Oral Presentation)</b></span></p>
                <!-- <br/> -->
                <a href="https://arxiv.org/pdf/1910.13395.pdf">PDF</a> |
                <a href="http://pair.stanford.edu/cavin/">Website</a> |
                <a href="http://ai.stanford.edu/blog/cavin/">Blog</a> |
                <a href="https://github.com/StanfordVL/robovat/">Environment</a> |
                <a href="https://github.com/StanfordVL/cavin/">Code</a> |
                <a href="bibtex/fang2019cavin.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/smt.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang,</span>
                  Alexander Toshev,
                  Li Fei-Fei, 
                  Silvio Savarese
                </span>
                <br/>
                CVPR 2019
                <br/>
                <a href="https://arxiv.org/pdf/1903.03878.pdf">PDF</a> |
                <a href="https://sites.google.com/view/scene-memory-transformer">Website</a> |
                <a href="bibtex/fang2019smt.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/tognet.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang,</span>
                  Yuke Zhu,
                  Animesh Garg,
                  Andrey Kurenkov,
                  Viraj Mehta,
                  Li Fei-Fei,
                  Silvio Savarese
                </span>
                <br/>
                RSS 2018 (Journal version in IJRR 2019)
                <br/>
                <a href="https://arxiv.org/pdf/1806.09266.pdf">PDF (RSS 2018)</a> |
                <a href="https://doi.org/10.1177/0278364919872545">PDF (IJRR 2019)</a> |
                <a href="https://sites.google.com/view/task-oriented-grasp/">Website</a> |
                <a href="https://www.youtube.com/watch?v=vYExCFeKXhw&feature=youtu.be">Video</a> |
                <a href="bibtex/fang2018tog.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/mtda.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from Simulation
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>,
                  Yunfei Bai, 
                  Stefan Hinterstoisser, 
                  Silvio Savarese, 
                  Mrinal Kalakrishnan
                </span>
                <br/>
                ICRA 2018
                <br/>
                <a href="https://arxiv.org/pdf/1710.06422.pdf">PDF</a> |
                <a href="https://sites.google.com/view/multi-task-domain-adaptation/">Website</a> |
                <a href="https://www.youtube.com/watch?v=BR5bgPxjvRM">Video</a> |
                <a href="bibtex/fang2018mtda.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/demo2vec.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Demo2Vec: Reasoning Object Affordances from Online Videos
                </span>
                <br/>
                <span id="author">
                  <span id="author-kuan">Kuan Fang</span>*,
                  Te-Lin Wu*, 
                  Daniel Yang, 
                  Silvio Savarese, 
                  Joseph J. Lim 
                  (* indicates equal contribution)
                </span>
                <br/>
                CVPR 2018
                <br/>
                <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fang_Demo2Vec_Reasoning_Object_CVPR_2018_paper.pdf">PDF</a> |
                <a href="https://sites.google.com/view/demo2vec/">Website</a> |
                <a href="https://github.com/kuanfang/opra">Code</a> |
                <a href="bibtex/fang2018demo2vec.bib">BibTex</a>
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/ran.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Learning with Noisy labels via Self-supervised Adversarial Noisy Masking
                </span>
                <br/>
                <span id="author">Yuanpeng Tu*, Boshen Zhang*, Yuxi Li*, Liang Liu, Jian Li, Yabiao Wang, Chengjie Wang, Cairong Zhao.
                </span>
                <br/>
                CVPR 2023
                <br/>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Learning_With_Noisy_Labels_via_Self-Supervised_Adversarial_Noisy_Masking_CVPR_2023_paper.pdf">PDF</a> |
                <a href="https://www.youtube.com/watch?v=Y-auFnnQ-lw">Video</a> |
              </p>
            </td>
          </tr>

          <tr>
            <td id="publications-image">
              <img src="papers/images/delay.png"/>
            </td>
            <td id="publications-info">
              <p>
                <span id="paper">
                  Domain Camera Adaptation and Collaborative Multiple Feature Clustering for Unsupervised Person Re-ID
                </span>
                <br/>
                <span id="author">Yuanpeng Tu
                </span>
                <br/>
                ACMMM 2022 Workshop
                <br/>
                <a href="https://dl.acm.org/doi/abs/10.1145/3552458.3556446">PDF</a> |
              </p>
            </td>
          </tr>

        </table>
      </div>

      <div class="section">
        <h1>Awards and Honors</h1><br/>

        <ul>
          <li>Shanghai Outstanding Graduates (2023)</li>
          <li>National Scholarship for Graduate Students (Rank 1/350)(2021)</li>
          <li>Scholarship for outstanding Graduate Students (Rank 6/350)(2022)</li>
          <li>Outstanding Graduate Students Certificate(2021/2022)</li>
          <li>Third Prize Scholarship for outstanding students, Tongji University, 2017 & 2018 & 2019</li>

        </ul>
      </div>


      <div class="section">
        <h1>Experiences</h1><br/>

        <ul>
          <li>03/2023 - 05/2023, Research Intern at QCraft.</li>
          <li>08/2021 - 03/2023, Research Intern at Shanghai Tencent Youtu Lab.</li>
        </ul>
      </div>


      <div class="section">
        <h1>Contests</h1><br/>

        <ul>
          <li><b>3rd Place</b>, Out-of-distribution Detection Competition in ECCV2022.</li>
          <li>Second Prize of "Huawei Cup" China Postgraduate Mathematical Modeling Competition(2020).</li>

        </ul>
      </div>


      <div class="section">
        <h1>Sevices</h1><br/>

        <ul>
          <li>Reviewer for CVPR 2023, ICCV 2023</li>
        </ul>
      </div>

    </div>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <p align="left">
            <font size="2">
              <a href="https://people.eecs.berkeley.edu/~barron/">Website template courtesy</a>
            </font>
          </p>
        </td>
      </tr>
    </table>

    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MQ25LJL"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

	</body>
</html>
